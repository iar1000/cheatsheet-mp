\section{Intro}

\textbf{Perceptron:} while $\exists x. w^T x > 0 \neq y$ do $w = w + \eta (y - \why) x$\\
\begin{comment}
	\Note{If the data is separable, the algorithm converges in finite time} \\
\end{comment}

\textbf{MLP:} $\forall l \quad x^{(l)} = \sigma((w^{(l)})^T x^{(l-1)} + b^{(l)}), f(x;w,b) = x^{(L)}$\\
\begin{comment}
	\textbf{Activation function:} If the activation function is linear, than the MLP is an affine transformation with a perciluar parameterization.\\
\end{comment}

\textbf{Sigmoid:} $\sigma(x) = \frac{1}{1 + e^{-x}} = \frac{e^x}{e^x + 1}$, $\nabla: \sigma(x) \cdot (1 - \sigma(x))$\\

\textbf{Softmax:} $softmax(x_i) = \exp(x_i) / \sum_j \exp(x_j)$\\
\begin{comment}
	\textbf{Requirements:} 
	Output must be positive \\
	output must be between $[0,1]$\\
	sum of all outputs must be 1, e.g. $\sum^M_i softmax(x_i) = 1$\\
\end{comment}

\textbf{tanh:} $\nabla: 1 - tanh(x)^2$\\

\textbf{MLE:} maximize $\log L(\theta) = \log \prod p(x_i| \theta) = \sum \log p(x_i | \theta)$\\
\begin{comment}
	\Note{Least squares and CE are ML estimators}\\
	\textbf{Algorithm:}\\
	Write down probability distribution\\
	Decompose into per sample probability\\
	Minimize negative log likelihood\\
\end{comment}

\textbf{$L_{CE}$:} $- \frac{1}{N} \sum y_i \log (\sigma(w^T x_i)) + (1 - y_i) \log (1 - \sigma(w^T x_i))$ (MLE)\\
\begin{comment}
	\Note{Cross-entropy loss is a maximum likelihood estimator}\\
	\textbf{Assumption:} y's are Bernoulli $Ber(\sigma(w^T x))$ distributed.
	From there we maximise the weights over the probability:\\
	 $P(D|w) = \prod^N \sigma(w^T x_i)^{y_i} (1 - \sigma(w^T x_i))^{1 - y_i}$.\\
	Taking the negative log-likelihood results in the given loss.\\
\end{comment}

\textbf{Universal Approximation:} $\exists g(x) = \sum v_i \sigma(w_i^T x + b_i) \approx f(x)$ and $\abs{g(x) - f(x)} < \epsilon$, $\sigma$ non-const, bounded, continus\\
\begin{comment}
	\textbf{Intuition:} We can create bumps with only two hidden neurons, having a lot of neurons can create many bumps to approximate any function.\\
 	\Note{$\sigma: \R \rightarrow \R$ must be non-constant, bounded and continous}\\
 	\Note{$f \in [0,1]^m$, e.g. must be in the m-dimensional hypercube}\\
\end{comment}

\textbf{SGD:} $\theta = \theta - \eta \nabla C(\theta)$, \textbf{Batch:} gradient avg. of whole dataset\\
\begin{comment}
	SGD has more fluctuations and will never reach the minimum, but dances around it.
	The convergence is usually faster though, since it updates the parameters more often\\
	Batch is doing one update per epoch, inefficient if there are a lot of samples in the dataset\\
\end{comment}



 
 

